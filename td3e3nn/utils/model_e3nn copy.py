import torch
from torch import nn
import torch.nn.functional as F
from torch_geometric.data import Data
from e3nn import o3
import torch_scatter
import e3nn
from torch_scatter import scatter

torch.set_default_dtype(torch.float64)

from typing import Dict, Union

import torch
from torch_geometric.data import Data
from torch_cluster import radius_graph

from e3nn import o3
from e3nn.math import soft_one_hot_linspace
from e3nn.nn import Gate
from e3nn.nn.models.gate_points_2101 import Convolution, smooth_cutoff, tp_path_exists

class CustomCompose(torch.nn.Module):
    def __init__(self, first, second):
        super().__init__()
        self.first = first
        self.second = second
        self.irreps_in = self.first.irreps_in
        self.irreps_out = self.second.irreps_out

    def forward(self, *input):
        x = self.first(*input)
        self.first_out = x.clone()
        x = self.second(x)
        self.second_out = x.clone()
        return x


class Network(torch.nn.Module):

    def __init__(
        self,
        irreps_in,
        irreps_out,
        irreps_node_attr,
        layers,
        mul,
        lmax,
        max_radius,
        number_of_basis=10,
        radial_layers=1,
        radial_neurons=100,
        num_neighbors=1.,
        num_nodes=1.,
        reduce_output=True,
        act = {
            1: torch.nn.functional.silu,
            -1: torch.tanh,
        },
        act_gates = {
            1: torch.sigmoid,
            -1: torch.tanh,
        }
    ) -> None:
        super().__init__()
        self.mul = mul
        self.lmax = lmax
        self.max_radius = max_radius
        self.number_of_basis = number_of_basis
        self.num_neighbors = num_neighbors
        self.num_nodes = num_nodes
        self.reduce_output = reduce_output

        self.irreps_in = o3.Irreps(irreps_in) if irreps_in is not None else None
        self.irreps_hidden = o3.Irreps([(self.mul, (l, p)) for l in range(lmax + 1) for p in [-1, 1]])
        self.irreps_out = o3.Irreps(irreps_out)
        self.irreps_node_attr = o3.Irreps(irreps_node_attr) if irreps_node_attr is not None else o3.Irreps("0e")
        self.irreps_edge_attr = o3.Irreps.spherical_harmonics(lmax)

        self.input_has_node_in = (irreps_in is not None)
        self.input_has_node_attr = (irreps_node_attr is not None)

        irreps = self.irreps_in if self.irreps_in is not None else o3.Irreps("0e")

        act = act
        act_gates = act_gates

        self.layers = torch.nn.ModuleList()

        for _ in range(layers):
            irreps_scalars = o3.Irreps([(mul, ir) for mul, ir in self.irreps_hidden if ir.l == 0 and tp_path_exists(irreps, self.irreps_edge_attr, ir)])
            irreps_gated = o3.Irreps([(mul, ir) for mul, ir in self.irreps_hidden if ir.l > 0 and tp_path_exists(irreps, self.irreps_edge_attr, ir)])
            ir = "0e" if tp_path_exists(irreps, self.irreps_edge_attr, "0e") else "0o"
            irreps_gates = o3.Irreps([(mul, ir) for mul, _ in irreps_gated])

            gate = Gate(
                irreps_scalars, [act[ir.p] for _, ir in irreps_scalars],  # scalar
                irreps_gates, [act_gates[ir.p] for _, ir in irreps_gates],  # gates (scalars)
                irreps_gated  # gated tensors
            )
            conv = Convolution(
                irreps,
                self.irreps_node_attr,
                self.irreps_edge_attr,
                gate.irreps_in,
                number_of_basis,
                radial_layers,
                radial_neurons,
                num_neighbors
            )
            irreps = gate.irreps_out
            self.layers.append(CustomCompose(conv, gate))

        self.layers.append(
            Convolution(
                irreps,
                self.irreps_node_attr,
                self.irreps_edge_attr,
                self.irreps_out,
                number_of_basis,
                radial_layers,
                radial_neurons,
                num_neighbors
            )
        )

    def preprocess(self, data: Union[Data, Dict[str, torch.Tensor]]) -> torch.Tensor:
        if 'batch' in data:
            batch = data['batch']
        else:
            batch = data['pos'].new_zeros(data['pos'].shape[0], dtype=torch.long)

        if 'edge_index' in data:
            edge_src = data['edge_index'][0]  # edge source
            edge_dst = data['edge_index'][1]  # edge destination
            edge_vec = data['edge_vec']

        else:
            edge_index = radius_graph(data['pos'], self.max_radius, batch)
            edge_src = edge_index[0]
            edge_dst = edge_index[1]
            edge_vec = data['pos'][edge_src] - data['pos'][edge_dst]

        return batch, edge_src, edge_dst, edge_vec

    def forward(self, data: Union[Data, Dict[str, torch.Tensor]]) -> torch.Tensor:
        """evaluate the network
        Parameters
        ----------
        data : `torch_geometric.data.Data` or dict
            data object containing
            - ``pos`` the position of the nodes (atoms)
            - ``x`` the input features of the nodes, optional
            - ``z`` the attributes of the nodes, for instance the atom type, optional
            - ``batch`` the graph to which the node belong, optional
        """
        batch, edge_src, edge_dst, edge_vec = self.preprocess(data)
        edge_sh = o3.spherical_harmonics(self.irreps_edge_attr, edge_vec, True, normalization='component')
        edge_length = edge_vec.norm(dim=1)
        edge_length_embedded = soft_one_hot_linspace(
            x=edge_length,
            start=0.0,
            end=self.max_radius,
            number=self.number_of_basis,
            basis='gaussian',
            cutoff=False
        ).mul(self.number_of_basis**0.5)
        edge_attr = smooth_cutoff(edge_length / self.max_radius)[:, None] * edge_sh

        if self.input_has_node_in and 'x' in data:
            assert self.irreps_in is not None
            x = data['x']
        else:
            assert self.irreps_in is None
            x = data['pos'].new_ones((data['pos'].shape[0], 1))

        if self.input_has_node_attr and 'z' in data:
            z = data['z']
        else:
            assert self.irreps_node_attr == o3.Irreps("0e")
            z = data['pos'].new_ones((data['pos'].shape[0], 1))

        #Added by myself
        data["edge_attr"] = edge_attr
        data["edge_length_embedded"] = edge_length_embedded

        for lay in self.layers:
            x = lay(x, z, edge_src, edge_dst, edge_attr, edge_length_embedded)

        if self.reduce_output:
            return scatter(x, batch, dim=0).div(self.num_nodes**0.5)
        else:
            return x


class PeriodicNetwork_Pi(Network):
    r"""The class for the Actor, based on Tensor-Field Networks in the e3nn library

    Parameters
    ----------
    em_dim : int
        Number of neurons in the embedding layer for the force norm.

    noise_clip : float
        Limit for absolute value of target policy smoothing noise.

    scaled : bool
        Flag controlling whether to predict actions relative to the unit cell.
    """

    def __init__(self, em_dim, noise_clip = 0.2, scaled = False, expl_mode = 'state', **kwargs):

        kwargs['reduce_output'] = False
        self.scaled = scaled

        super().__init__(**kwargs)
        self.em = nn.Linear(1, em_dim)
        self.noise_clip = noise_clip
        self.exploration_mode = expl_mode

    def forward(self, data, noise_scale = None) :
        data_copy = data.clone()
        if noise_scale is not None and self.exploration_mode == 'state':
            axis, angle = e3nn.o3.rand_axis_angle(1)
            angle *= 0.5*noise_scale
            angle = torch.clamp(angle, -self.noise_clip, self.noise_clip)
            rot_matrix = e3nn.o3.axis_angle_to_matrix(axis, angle).to(data_copy.forces_stack.device)
            data_copy.forces_stack = torch.matmul(data_copy.forces_stack, rot_matrix[0])

            epsilon = (2*torch.rand(data_copy.forces_norm.shape[0],1)-1)*noise_scale
            epsilon = torch.clamp(epsilon, -self.noise_clip, self.noise_clip).to(data_copy.forces_stack.device)
            data_copy.forces_norm *= (1+epsilon)

        forces_ampl = F.leaky_relu(self.em(data_copy.forces_norm))
        data_copy.x = torch.hstack([data_copy.x, data_copy.forces_stack, forces_ampl])
        output = super().forward(data_copy)

        if noise_scale is not None and self.exploration_mode == 'action':
            epsilon = torch.randn_like(data_copy.x) * noise_scale
            epsilon = torch.clamp(epsilon, -self.noise_clip, self.noise_clip)
            data_copy.x += epsilon

        if self.scaled:
            output = torch.tanh(output)
        return Data(x = output)

class PeriodicNetwork_Q(Network):
    r"""The class for the Critic, based on Tensor-Field Networks in the e3nn library

    Parameters
    ----------
    em_dim : int
        Number of neurons in the embedding layer for the force norm.

    """

    def __init__(self, em_dim, **kwargs):
        # override the `reduce_output` keyword to instead perform an averge over atom contributions

        kwargs['reduce_output'] = False

        super().__init__(**kwargs)

        self.em = nn.Linear(1, em_dim)
        self.em_act = nn.Linear(1, em_dim)

    def forward(self, data, actions) -> torch.Tensor:

        data_copy = data.clone()
        action = actions.x
        action_norm = action.norm(dim=1)
        action_norm_cor = action_norm + 1*(action_norm==0)
        action_stack = action/action_norm_cor[:, None]
        action_ampl = F.leaky_relu(self.em_act(action_norm.unsqueeze(1)))

        forces_ampl = F.leaky_relu(self.em(data_copy.forces_norm))
        data_copy.x = torch.hstack([data_copy.x, data_copy.forces_stack, forces_ampl, action_stack, action_ampl])

        output = super().forward(data_copy)
        # if pool_nodes was set to True, use scatter_mean to aggregate
        output = torch_scatter.scatter_mean(output, data_copy.batch, dim=0)  # take mean over atoms per example

        return torch.squeeze(output, -1)